{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Callable, Iterable\n",
        "from dataclasses import dataclass, field\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "xzlSaJ0mPIL3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "N5mGq9MLPPH3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ORmm1tE3VTZ"
      },
      "outputs": [],
      "source": [
        "import telebot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Параметры"
      ],
      "metadata": {
        "id": "EyvStBMXPYBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Params:\n",
        "    root_dir = \"/content/drive/My Drive/MIPT/NLPGen/HW1\"\n",
        "    joey_dataset_path = os.path.join(root_dir, \"joey.csv\")\n",
        "\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    sbert_softmax_model_path = os.path.join(root_dir, \"sbert_softmax_lr_2e-6\")\n",
        "\n",
        "    token = \"7165553675:AAGUILrqnq55Vl-08Kh4ACTA2mt3NEYdyH4\"\n",
        "\n",
        "params = Params()"
      ],
      "metadata": {
        "id": "N_dZcTiCOdkB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Модель"
      ],
      "metadata": {
        "id": "AljRaxziPtfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pool(token_embeds: torch.tensor, attention_mask: torch.tensor) -> torch.tensor:\n",
        "    in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
        "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def encode(input_texts: list[str], tokenizer: AutoTokenizer, model: AutoModel, device: str = \"cpu\"\n",
        ") -> torch.tensor:\n",
        "\n",
        "    model.eval()\n",
        "    tokenized_texts = tokenizer(input_texts, max_length=128,\n",
        "                                padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "    token_embeds = model(tokenized_texts[\"input_ids\"].to(device),\n",
        "                         tokenized_texts[\"attention_mask\"].to(device)).last_hidden_state\n",
        "    pooled_embeds = mean_pool(token_embeds, tokenized_texts[\"attention_mask\"].to(device))\n",
        "    return pooled_embeds"
      ],
      "metadata": {
        "id": "4Rrw4dkiPW9T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sbert(torch.nn.Module):\n",
        "    def __init__(self, max_length: int = 128):\n",
        "        super().__init__()\n",
        "        self.max_length = max_length\n",
        "        self.bert_model = AutoModel.from_pretrained(params.model_name)\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(params.model_name)\n",
        "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size * 3, 3)\n",
        "\n",
        "    def forward(self, data: datasets.arrow_dataset.Dataset) -> torch.tensor:\n",
        "        premise_input_ids = data[\"premise_input_ids\"].to(device)\n",
        "        premise_attention_mask = data[\"premise_attention_mask\"].to(device)\n",
        "        hypothesis_input_ids = data[\"hypothesis_input_ids\"].to(device)\n",
        "        hypothesis_attention_mask = data[\"hypothesis_attention_mask\"].to(device)\n",
        "\n",
        "        out_premise = self.bert_model(premise_input_ids, premise_attention_mask)\n",
        "        out_hypothesis = self.bert_model(hypothesis_input_ids, hypothesis_attention_mask)\n",
        "        premise_embeds = out_premise.last_hidden_state\n",
        "        hypothesis_embeds = out_hypothesis.last_hidden_state\n",
        "\n",
        "        pooled_premise_embeds = mean_pool(premise_embeds, premise_attention_mask)\n",
        "        pooled_hypotheses_embeds = mean_pool(hypothesis_embeds, hypothesis_attention_mask)\n",
        "\n",
        "        embeds =  torch.cat([pooled_premise_embeds, pooled_hypotheses_embeds,\n",
        "                             torch.abs(pooled_premise_embeds - pooled_hypotheses_embeds)],\n",
        "                            dim=-1)\n",
        "        return self.linear(embeds)"
      ],
      "metadata": {
        "id": "rXIHhzzhPs3y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Подготовка"
      ],
      "metadata": {
        "id": "sd-yUFUkPxeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_vMsIIEMPwxF",
        "outputId": "c0261907-e314-4db0-861f-54a93af2c901"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sbert_softmax_model = Sbert().to(device)\n",
        "sbert_softmax_model.bert_model.from_pretrained(params.sbert_softmax_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzERyKOfP34w",
        "outputId": "3a78ccf8-17d8-4088-b750-0f002d71d48a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joey_df = pd.read_csv(params.joey_dataset_path)"
      ],
      "metadata": {
        "id": "la7mKRXiP5td"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bi_answer(question, context, answers, tokenizer, model, device=\"cuda\"):\n",
        "    # Объединяем вопрос и контекст\n",
        "    combined_text = question + \" [SEP] \" + context\n",
        "\n",
        "    # Создаем массив с комбинированным текстом вопроса и контекста в качестве первого элемента, а затем добавляем все ответы\n",
        "    texts_to_encode = [combined_text] + answers\n",
        "\n",
        "    # Кодируем весь массив текстов за один вызов функции encode\n",
        "    embeddings = encode(texts_to_encode, tokenizer, model, device).cpu().detach().numpy()\n",
        "\n",
        "    # Вычисляем косинусную близость между эмбеддингом вопроса-контекста (первым элементом массива эмбеддингов)\n",
        "    # и эмбеддингами каждого из ответов (остальные элементы массива)\n",
        "    question_context_embedding = embeddings[0].reshape(1, -1)\n",
        "    answers_embeddings = embeddings[1:]\n",
        "    similarities = cosine_similarity(question_context_embedding, answers_embeddings).flatten()\n",
        "\n",
        "    # Находим индекс ответа с наибольшей косинусной близостью\n",
        "    best_answer_index = similarities.argmax()\n",
        "\n",
        "    # Возвращаем наиболее подходящий ответ, его косинусную близость и аккумулированный контекст\n",
        "    best_answer = answers[best_answer_index]\n",
        "    updated_context = best_answer + \" [SEP] \" + question  # Аккумулируем вопрос с лучшим ответом\n",
        "    return best_answer, similarities[best_answer_index], updated_context"
      ],
      "metadata": {
        "id": "r0m-GahTP8Zg"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_joey_reply(question, context, joey_df, tokenizer, model, device, num_tries=5):\n",
        "    best_global_answer = None\n",
        "    best_global_score = -float('inf')\n",
        "\n",
        "    for _ in range(num_tries):\n",
        "        np.random.seed(int(time.time()))\n",
        "        sample_answers = joey_df[\"positive_answer\"].sample(n=100, replace=False).tolist()\n",
        "        best_answer, best_score, _ = get_bi_answer(question, context, sample_answers, tokenizer, model, device)\n",
        "        if best_score > best_global_score:\n",
        "            best_global_score = best_score\n",
        "            best_global_answer = best_answer\n",
        "\n",
        "    return best_global_answer"
      ],
      "metadata": {
        "id": "m3o-S5NjUr6D"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Запускаем бота"
      ],
      "metadata": {
        "id": "HPD_ZM1SQ7Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot(params.token)\n",
        "\n",
        "context = \"\"\n",
        "\n",
        "@bot.message_handler(commands=[\"start\"])\n",
        "def start(m, res=False):\n",
        "    global context\n",
        "    contex = \"Hi! I'm Joe!\"\n",
        "    bot.send_message(m.chat.id, context)\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=[\"text\"])\n",
        "def process_message(message):\n",
        "    global context\n",
        "    reply = get_joey_reply(\n",
        "        message.text,\n",
        "        context,\n",
        "        joey_df,\n",
        "        sbert_softmax_model.bert_tokenizer,\n",
        "        sbert_softmax_model.bert_model,\n",
        "        device\n",
        "    )\n",
        "    context = random.choice([message.text, reply])\n",
        "    bot.send_message(message.chat.id, reply)"
      ],
      "metadata": {
        "id": "m9GYSN9wQv7G"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"https://t.me/JoeyTheRetrieverBot\")\n",
        "bot.polling(none_stop=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4XYdK3WRQUW",
        "outputId": "00dc1b5c-bd98-4e0a-d6c7-b93ceed73b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://t.me/JoeyTheRetrieverBot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCtBU5PrSwk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}