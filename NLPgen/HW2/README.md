# ДЗ №2 - Чат-бот Джо Триббиани

- Ссылка на веб-сервис (доступна в течение 72 часов, буду периодически обновлять), запросы вводятся в поле "ваше сообщение" и отправляются нажатием Enter - https://0b16f7c82e7a16bbe6.gradio.live
- Телеграм-бот (на данный момент выключен в пользу веб-сервиса) - https://t.me/JoeyGeneratorBot

# 1. Файлы 

В корневой папке лежат python-файлы: 
1. app.py - для запуска веб-сервиса на базе Gradio (асинхронный режим вывода).
2. telegram_bot.py - решение на основе Telegram API (синхронный режим вывода).

В папке weights лежат веса обученной модели.

В папке notebooks находятся:
1. NLPGen_HW2_training.ipynb - ноутбук, содержащий код и отчет об обучении модели.
2. NLPGen_HW2_bot.ipynb - ноутбук, содержащий тестовый код запуска чат-бота на Gradio и Telegram API.

# 2. Описание модели

Модель PeftModelForCausalLM представляет собой адаптивную версию языковой модели, базирующуюся на архитектуре LLaMA (Llama For Causal Language Modeling), дополненную механизмом LoRA (Low-Rank Adaptation) для эффективной тонкой настройки. LoRA позволяет специализировать модель под конкретные задачи, обновляя лишь небольшую часть параметров, что делает процесс обучения более быстрым и экономичным, сохраняя при этом общую структуру и веса предварительно обученной модели.

### Ключевые компоненты модели:
- LoraModel: Обертка, интегрирующая LoRA с базовой моделью LLaMA.
- LlamaForCausalLM: Специализированная для генерации текста модель на базе LLaMA.
- LoRA: Техника адаптации, позволяющая изменять небольшое число параметров для специализации модели.
- LlamaDecoderLayer и LlamaAttention: Основные блоки, состоящие из слоев внимания и многослойного перцептрона (MLP), обогащенные LoRA-адаптациями.

### Основа - Tiny LLaMA (tinyllama-15M):
В основе PeftModelForCausalLM лежит модель LLaMA, в частности, вариант Tiny LLaMA с ~15 миллионами параметров. Tiny LLaMA представляет собой уменьшенную версию мощных моделей на основе трансформеров, предназначенную для задач обработки естественного языка. Несмотря на относительно малый размер, Tiny LLaMA способна демонстрировать высокое качество генерации текста и понимания языка благодаря эффективной архитектуре и предварительному обучению на большом корпусе текстов. Интеграция с LoRA позволяет дополнительно адаптировать модель под конкретные задачи с минимальными затратами на переобучение.

# 3. Асинхронный вывод с Gradio
Несмотря на то что в Gradio присутствует специализированный ChatInterface, он не поддерживает асинхронный вывод (стриминг), поэтому для тестирования асинхронного вывода я использовал обычный Gradio Interface.
К сожалению, ограничения интерфейса Gradio не позволяют мне автоматически заполнять поле контекста предыдущей фразой собеседника.
Запуск `python app.py`.

# 4. Телеграм-бот
Телегам-бот использует синхронный вывод.
Запуск `python telegram_bot.py`.