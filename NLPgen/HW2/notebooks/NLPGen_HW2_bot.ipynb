{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers telebot accelerate peft sentencepiece"
      ],
      "metadata": {
        "id": "Hvk6c2M-7IU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "kfCFEw3SRTQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import base64\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import Union\n",
        "from dataclasses import dataclass, field"
      ],
      "metadata": {
        "id": "do9erVV51GNX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "NuTECuvqB57t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "lkdqn05f7UUe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "KMJSAh0A4L5L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import transformers\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "ZFvg6yVD6y_9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot"
      ],
      "metadata": {
        "id": "BbG5LdVK1GCl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xor_decrypt_from_str(encrypted_str, key):\n",
        "    encrypted_bytes = base64.b64decode(encrypted_str.encode('utf-8'))\n",
        "    key_bytes = key.encode('utf-8')\n",
        "    full_key_bytes = (key_bytes * (len(encrypted_bytes) // len(key_bytes) + 1))[:len(encrypted_bytes)]\n",
        "    decrypted_bytes = bytes([encrypted_byte ^ key_byte for encrypted_byte, key_byte in zip(encrypted_bytes, full_key_bytes)])\n",
        "    decrypted_str = decrypted_bytes.decode('utf-8')\n",
        "\n",
        "    return decrypted_str\n",
        "\n",
        "encrypted_telegram_api_key = \"BggEAgMHCQUAAgp2cXgEc19oVwdnVgBWZmBbY0ZAYgYCVQFjQXp8XQFcV2AOZQ==\"\n",
        "telegram_api_key = xor_decrypt_from_str(encrypted_telegram_api_key, \"007\")"
      ],
      "metadata": {
        "id": "uVl3N_Zu2T00"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Params:\n",
        "    repo_url = \"https://github.com/dmitryilyn/MIPT.git\"\n",
        "    temp_path = \"/tmp/MIPT_repo\"\n",
        "    git_weights_dir = \"NLPgen/HW2/weights\"\n",
        "    git_utils_dir = \"NLPgen/HW2/utils\"\n",
        "    weights_path = \"weights\"\n",
        "    utils_path = \"utils\"\n",
        "\n",
        "    base_model = \"nickypro/tinyllama-15M\"\n",
        "    tokenizer_name = \"hf-internal-testing/llama-tokenizer\"\n",
        "\n",
        "    device = \"cuda\"\n",
        "\n",
        "    load_in_8bit = False\n",
        "    torch_dtype =  torch.float32\n",
        "    device_map: str = field(init=False)\n",
        "    low_cpu_mem_usage = True\n",
        "\n",
        "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "    ddp = world_size != 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.ddp:\n",
        "            self.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "        else:\n",
        "            self.device_map = \"auto\"\n",
        "\n",
        "\n",
        "params = Params()"
      ],
      "metadata": {
        "id": "lLumVpDq29FF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем модель"
      ],
      "metadata": {
        "id": "3o5JDeuK23_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run([\"git\", \"clone\", params.repo_url, params.temp_path])\n",
        "shutil.copytree(os.path.join(params.temp_path, params.git_weights_dir), params.weights_path)\n",
        "shutil.copytree(os.path.join(params.temp_path, params.git_utils_dir), params.utils_path)\n",
        "shutil.rmtree(params.temp_path)"
      ],
      "metadata": {
        "id": "gwzRMSfURBi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.callbacks import Iteratorize, Stream\n",
        "from utils.prompter import Prompter"
      ],
      "metadata": {
        "id": "lD4aiOiq6TuR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompter = Prompter()\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(params.tokenizer_name)\n",
        "\n",
        "joey_model = LlamaForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=params.base_model,\n",
        "    load_in_8bit=params.load_in_8bit,\n",
        "    torch_dtype=params.torch_dtype,\n",
        "    device_map=params.device_map,\n",
        "    low_cpu_mem_usage=params.low_cpu_mem_usage,\n",
        ")\n",
        "joey_model = PeftModel.from_pretrained(\n",
        "    joey_model,\n",
        "    params.weights_path,\n",
        "    torch_dtype=params.torch_dtype,\n",
        "    device_map={'': 0},\n",
        ")\n",
        "joey_model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    joey_model = torch.compile(joey_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq84JI0y6TxE",
        "outputId": "c2e0c93a-479d-42d2-e787-7a50b1561914"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reply_async(\n",
        "    text,\n",
        "    context=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=512,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = prompter.generate_prompt(text, context)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(params.device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    generate_params = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generation_config\": generation_config,\n",
        "        \"return_dict_in_generate\": True,\n",
        "        \"output_scores\": True,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "    }\n",
        "\n",
        "    # Асинхронный вывод\n",
        "    def generate_with_callback(callback=None, **kwargs):\n",
        "        kwargs.setdefault(\"stopping_criteria\", transformers.StoppingCriteriaList())\n",
        "        kwargs[\"stopping_criteria\"].append(Stream(callback_func=callback))\n",
        "        with torch.no_grad():\n",
        "            joey_model.generate(**kwargs)\n",
        "\n",
        "    def generate_with_streaming(**kwargs):\n",
        "        return Iteratorize(generate_with_callback, kwargs, callback=None)\n",
        "\n",
        "    with generate_with_streaming(**generate_params) as generator:\n",
        "        for output in generator:\n",
        "            decoded_output = tokenizer.decode(output)\n",
        "\n",
        "            if output[-1] in [tokenizer.eos_token_id]:\n",
        "                break\n",
        "\n",
        "            yield prompter.get_response(decoded_output)\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "j11IVea57gwT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reply(\n",
        "    text,\n",
        "    context=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=512,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = prompter.generate_prompt(text, context)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(params.device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    generate_params = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generation_config\": generation_config,\n",
        "        \"return_dict_in_generate\": True,\n",
        "        \"output_scores\": True,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "    }\n",
        "\n",
        "    # Без стриминга инференса\n",
        "    with torch.no_grad():\n",
        "        generation_output = joey_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s, skip_special_tokens=True).strip()\n",
        "\n",
        "    return prompter.get_response(output)"
      ],
      "metadata": {
        "id": "5EvVu8SxQeHX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тест на Gradio с асинхронным выводом"
      ],
      "metadata": {
        "id": "lEhcSYIPBf8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Jogzt_Hk7g5y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "   fn=generate_reply_async,\n",
        "   inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=1,\n",
        "            label=\"Ваше сообщение\",\n",
        "            placeholder=\"Hi!\",\n",
        "        ),\n",
        "        gr.components.Textbox(\n",
        "            lines=1,\n",
        "            label=\"Контекст\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.1, label=\"Температура\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=0, maximum=1, value=0.75, label=\"Top p\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=0, maximum=100, step=1, value=40, label=\"Top k\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=4, step=1, value=4, label=\"Beams\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=1024, step=1, value=512, label=\"Максимальное число токенов\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=5,\n",
        "            label=\"Ответ\",\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "iface.queue()\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "1JJMU1mkHfhB",
        "outputId": "6cb44b75-3663-40d9-a72d-38de6c63334f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a96d1846cf728e0500.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a96d1846cf728e0500.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a96d1846cf728e0500.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oErmH7nB28I",
        "outputId": "e88334ec-734a-4f95-ebd7-0fb847c83e39"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Запускаем бота"
      ],
      "metadata": {
        "id": "HPD_ZM1SQ7Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xRE-u6ABQuIG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot(telegram_api_key)\n",
        "\n",
        "context = \"\"\n",
        "\n",
        "@bot.message_handler(commands=[\"start\"])\n",
        "def start(m, res=False):\n",
        "    global context\n",
        "    contex = \"Hi! I'm Joe!\"\n",
        "    bot.send_message(m.chat.id, context)\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=[\"text\"])\n",
        "def process_message(message):\n",
        "    global context\n",
        "    reply = generate_reply(\n",
        "        message.text,\n",
        "        context\n",
        "    )\n",
        "    context = reply\n",
        "    bot.send_message(message.chat.id, reply)"
      ],
      "metadata": {
        "id": "m9GYSN9wQv7G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"https://t.me/JoeyGeneratorBot\")\n",
        "bot.polling(none_stop=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4XYdK3WRQUW",
        "outputId": "52f7a2e7-e727-4478-cae1-f2aed8ef8a08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://t.me/JoeyGeneratorBot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCtBU5PrSwk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}